{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tacotron2 and WaveNet.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Laxmaan/Emotional-Speech-Synthesis/blob/main/Tacotron2_and_WaveNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syjMmwOEa-uk"
      },
      "source": [
        "# Tacotron2: WaveNet-basd text-to-speech demo\n",
        "\n",
        "- Tacotron2 (mel-spectrogram prediction part): https://github.com/Rayhane-mamah/Tacotron-2\n",
        "- WaveNet: https://github.com/r9y9/wavenet_vocoder\n",
        "\n",
        "This is a proof of concept for Tacotron2 text-to-speech synthesis. Models used here were trained on [LJSpeech dataset](https://keithito.com/LJ-Speech-Dataset/).\n",
        "\n",
        "**Notice**: The waveform generation is super slow since it implements naive autoregressive generation. It doesn't use parallel generation method described in [Parallel WaveNet](https://arxiv.org/abs/1711.10433). \n",
        "\n",
        "**Estimated time to complete**: 2 ~ 3 hours."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7R_1MpFc3Za"
      },
      "source": [
        "## Setup\n",
        "\n",
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-GaIwD9QYr5"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruoFh9NaIPwc"
      },
      "source": [
        "import os\n",
        "import os.path as osp\n",
        "from os.path import exists, join, expanduser\n",
        "from google.colab import drive\n",
        "import glob\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa\n",
        "drive.mount('/gdrive')\n",
        "base_path = os.path.join('/gdrive','My Drive','IST597')\n",
        "#base_path = expanduser('~')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlLC7Q7Us8go"
      },
      "source": [
        "\n",
        "\n",
        "os.chdir(base_path)\n",
        "\n",
        "wavenet_dir = \"wavenet_vocoder\"\n",
        "if not exists(wavenet_dir):\n",
        "  ! git clone https://github.com/r9y9/wavenet_vocoder\n",
        "  ! cd wavenet_vocoder && git checkout v0.1.1 && cd -\n",
        "    \n",
        "taco2_dir = \"Tacotron-2\"\n",
        "if not exists(taco2_dir):\n",
        "  ! git clone https://github.com/r9y9/$taco2_dir\n",
        "  ! cd $taco2_dir && git checkout -B wavenet3 origin/wavenet3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBFfji_Avluz"
      },
      "source": [
        "# Install dependencies'''\n",
        "! pip install -q -U \"tensorflow<=1.9.0\"\n",
        "! pip install -q -U \"numpy<1.16\"\n",
        "! pip install -q -U \"pysptk<=0.1.14\"\n",
        "! pip install -q -U keras==2.2.4\n",
        "os.chdir(join(base_path, taco2_dir))\n",
        "! pip install -q -r requirements.txt\n",
        "\n",
        "os.chdir(join(base_path, wavenet_dir))\n",
        "! pip install -q -e '.[train]'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15p8phXx6nxe"
      },
      "source": [
        "import torch\n",
        "import tensorflow\n",
        "import pysptk\n",
        "import numpy as np\n",
        "tensorflow.__version__, pysptk.__version__, np.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fZo1X7ac_Tp"
      },
      "source": [
        "### Download pretrained models\n",
        "\n",
        "#### Tacotron2 (mel-spectrogram prediction part)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sau06KhizkoD"
      },
      "source": [
        "os.chdir(join(base_path, taco2_dir))\n",
        "! mkdir -p logs-Tacotron\n",
        "if not exists(\"logs-Tacotron/pretrained\"):\n",
        "  ! curl -O -L \"https://www.dropbox.com/s/vx7y4qqs732sqgg/pretrained.tar.gz\"\n",
        "  ! tar xzvf pretrained.tar.gz\n",
        "  ! mv pretrained logs-Tacotron"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4tWl_hfdXdh"
      },
      "source": [
        "#### WaveNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2kwJ-t_ykXZ"
      },
      "source": [
        "os.chdir(join(base_path, wavenet_dir))\n",
        "wn_preset = \"20180510_mixture_lj_checkpoint_step000320000_ema.json\"\n",
        "wn_checkpoint_path = \"20180510_mixture_lj_checkpoint_step000320000_ema.pth\"\n",
        "\n",
        "if not exists(wn_preset):\n",
        "  !curl -O -L \"https://www.dropbox.com/s/0vsd7973w20eskz/20180510_mixture_lj_checkpoint_step000320000_ema.json\"\n",
        "if not exists(wn_checkpoint_path):\n",
        "  !curl -O -L \"https://www.dropbox.com/s/zdbfprugbagfp2w/20180510_mixture_lj_checkpoint_step000320000_ema.pth\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km1SAASEcIL6"
      },
      "source": [
        "## Input texts to be synthesized\n",
        "\n",
        "Choose your favorite sentences :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LeTMHHFdcmS"
      },
      "source": [
        "os.chdir(join(base_path, taco2_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tU1lz6PcbXut"
      },
      "source": [
        "%%bash\n",
        "cat << EOS > text_list.txt\n",
        "This is really awesome!\n",
        "This is text-to-speech online demonstration by Tacotron 2 and WaveNet.\n",
        "Thanks for your patience.\n",
        "Will you desert me in the desert?\n",
        "EOS\n",
        "\n",
        "cat text_list.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cq0Vk0H4gH_A"
      },
      "source": [
        "# Choose Modes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rp0jkxLKgHPV"
      },
      "source": [
        "make_mels = True\n",
        "make_waveforms = True\n",
        "process_ravdess = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9akhzMhbWe0"
      },
      "source": [
        "## Mel-spectrogram prediction by Tacoron2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0n4h5aa51dHS"
      },
      "source": [
        "def make_mel():\n",
        "    # Remove old files if exist\n",
        "    ! rm -rf tacotron_output\n",
        "    ! python synthesize.py --model='Tacotron' --mode='eval' \\\n",
        "    --hparams='symmetric_mels=False,max_abs_value=4.0,power=1.1,outputs_per_step=16' \\\n",
        "    --text_list=./text_list.txt\n",
        "\n",
        "if make_mels:\n",
        "    make_mel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXsE_ELsWa-A"
      },
      "source": [
        "# Preprocess RAVDESS AUDIO\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUZpn6eWryYj"
      },
      "source": [
        "if process_ravdess:\n",
        "    import warnings\n",
        "    warnings.filterwarnings('ignore')\n",
        "\n",
        "    os.chdir(join(base_path, wavenet_dir))\n",
        "    dataset_path = join(base_path,\"VAE\",\"data\")\n",
        "\n",
        "    x = glob.glob(dataset_path+\"/**/*.wav\")\n",
        "    print(x)\n",
        "    out_dir = join(base_path,\"wavenet_processed_data\")\n",
        "\n",
        "    len(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEvDVs34WeIC"
      },
      "source": [
        "if process_ravdess:\n",
        "    !python preprocess_latest.py --num_workers=8 --preset=20180510_mixture_lj_checkpoint_step000320000_ema.json wavallin \"$dataset_path\" \"$out_dir\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmyXfuGJJyef"
      },
      "source": [
        "### Convert mels to images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cx0zih2wJ_tT"
      },
      "source": [
        "def get_mel_img(path):\n",
        "    img = np.load(path)\n",
        "    \n",
        "    if img.shape[1] != 80:\n",
        "      img = np.swapaxes(img, 0, 1)\n",
        "    print(img.shape)\n",
        "    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
        "    img_int = np.interp(img,(0,4),(0,255))\n",
        "    img_int = np.uint8(np.round(img_int))\n",
        "    ax1.imshow(img)\n",
        "    \n",
        "    print(img_int.shape)\n",
        "    ax2.imshow(img_int)\n",
        "    \n",
        "    Image.fromarray(img_int).save(osp.splitext(path)[0]+'.png')\n",
        "    print(Image.fromarray(img_int).size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHNJQj8BJxEr"
      },
      "source": [
        "def img_to_npy(path):\n",
        "    img = Image.open(path).convert('L')\n",
        "    img_arr = np.array(img)\n",
        "    img_arr_new = np.interp(img_arr,(0,255),(0,1))\n",
        "    print(img_arr.shape)\n",
        "    print(img_arr_new.shape)\n",
        "    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
        "    \n",
        "    ax1.imshow(img_arr)\n",
        "    ax2.imshow(img_arr_new)\n",
        "    np.save(osp.splitext(path)[0]+'.npy',img_arr_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIfBYQ7mlOdl"
      },
      "source": [
        "mike_images = glob.glob(join(base_path,\"mike_images/**/*.png\"))\n",
        "print(mike_images)\n",
        "for img in mike_images:\n",
        "    img_to_npy(img)\n",
        "\n",
        "mike_mels = glob.glob(join(base_path,\"mike_images/**/*.npy\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF1mh1Jvdp0a"
      },
      "source": [
        "## Waveform synthesis by WaveNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY_MfE0m8Ese"
      },
      "source": [
        "import librosa.display\n",
        "import IPython\n",
        "from IPython.display import Audio\n",
        "import numpy as np\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTmp0T0G3lU0"
      },
      "source": [
        "os.chdir(join(base_path, wavenet_dir))\n",
        "\n",
        "# Setup WaveNet vocoder hparams\n",
        "from hparams import hparams\n",
        "with open(wn_preset) as f:\n",
        "    s = f.read()\n",
        "    #print(s)\n",
        "    hparams.parse_json(s)\n",
        "\n",
        "# Setup WaveNet vocoder\n",
        "from train import build_model\n",
        "from synthesis import wavegen\n",
        "import torch\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "model = build_model().to(device)\n",
        "\n",
        "print(\"Load checkpoint from {}\".format(wn_checkpoint_path))\n",
        "checkpoint = torch.load(wn_checkpoint_path,map_location=device)\n",
        "model.load_state_dict(checkpoint[\"state_dict\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkweGY4flvfV"
      },
      "source": [
        "\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "334X6oFK6Vf9"
      },
      "source": [
        "\n",
        "\n",
        "with open(\"../Tacotron-2/tacotron_output/eval/map.txt\") as f:\n",
        "  maps = f.readlines()\n",
        "maps = list(map(lambda x:x.strip().split(\"|\"), maps))\n",
        "# filter out invalid ones\n",
        "maps = list(filter(lambda x:len(x) == 2, maps))\n",
        "\n",
        "print(\"List of texts to be synthesized\")\n",
        "for idx, (text,_) in enumerate(maps):\n",
        "  print(idx, text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynaOnd2B9jjP"
      },
      "source": [
        "happy = [k for k in mike_mels if \"happy\" in k]\n",
        "angry = [k for k in mike_mels if \"angry\" in k]\n",
        "disgust = [k for k in mike_mels if \"disgust\" in k]\n",
        "\n",
        "mike_mels = [val for tup in zip(happy,disgust,angry) for val in tup]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9k4aGgQmlths"
      },
      "source": [
        "maps=[]\n",
        "for mel in mike_mels:\n",
        "    maps.append((osp.splitext(mel)[0],mel))\n",
        "\n",
        "print(maps[:2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaleFjoyiND_"
      },
      "source": [
        "### Waveform generation\n",
        "\n",
        "**Note**: This will takes hours to finish depending on the number and lenght of texts. Try short sentences first if you would like to see samples quickly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9BO7IES7Htp"
      },
      "source": [
        "waveforms = []\n",
        "\n",
        "for idx, (text, mel) in enumerate(maps):\n",
        "  print(\"\\n\", idx, text)\n",
        "  mel_path = mel#join(\"../Tacotron-2\", mel)\n",
        "  audio_pth = osp.splitext(mel_path)[0]+\".wav\"\n",
        "  if not os.path.exists(audio_pth):\n",
        "    c = np.load(mel_path)\n",
        "    print(c.shape)\n",
        "    if c.shape[1] != hparams.num_mels:\n",
        "        c=np.swapaxes(c, 0, 1)\n",
        "    print(c.max())\n",
        "    # Range [0, 4] was used for training Tacotron2 but WaveNet vocoder assumes [0, 1]\n",
        "    c = np.interp(c, (c.min(), c.max()), (0, 1))\n",
        "    \n",
        "    # Generate\n",
        "    waveform = wavegen(model, c=c, fast=True, tqdm=tqdm)\n",
        "    # Audio\n",
        "    IPython.display.display(Audio(waveform, rate=hparams.sample_rate))\n",
        "    waveforms.append(waveform)\n",
        "    # Save\n",
        "    audio_pth = osp.splitext(mel_path)[0]+\".wav\"\n",
        "    librosa.output.write_wav(audio_pth, waveform, hparams.sample_rate)\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HECnClUo9E97"
      },
      "source": [
        " '''\n",
        " function ConnectButton(){\n",
        "    console.log(\"Working\"); \n",
        "    document.querySelector(\"#top-menubar\").click() \n",
        "}\n",
        "setInterval(ConnectButton,60000);\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNG8oI4OiJkJ"
      },
      "source": [
        "## Summary: audio samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIyfhn0v9Ntg"
      },
      "source": [
        "for idx, (text, mel) in enumerate(maps):\n",
        "  print(idx, text)\n",
        "  IPython.display.display(Audio(waveforms[idx], rate=hparams.sample_rate))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3u697enwes0l"
      },
      "source": [
        "# Save Audio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PT1U2ufesQ_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0hc4ah-gMUa"
      },
      "source": [
        "For more information, please visit https://github.com/r9y9/wavenet_vocoder. More samples can  be  found at https://r9y9.github.io/wavenet_vocoder/. "
      ]
    }
  ]
}